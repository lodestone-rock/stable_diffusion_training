@@ -214,7 +214,7 @@
                 query_chunk_size = int(flatten_latent_dim)
 
             hidden_states = jax_memory_efficient_attention(
-                query_states, key_states, value_states, query_chunk_size=query_chunk_size, key_chunk_size=4096 * 4
+                query_states, key_states, value_states, query_chunk_size=query_chunk_size, key_chunk_size=flatten_latent_dim
             )
 
             hidden_states = hidden_states.transpose(1, 0, 2)
@@ -235,8 +235,8 @@
                 hidden_states = jnp.reshape(hidden_states, (b, -1, self.heads * self.dim_head))
             else:
                 hidden_states = jnp.einsum("b i j, b j d -> b i d", attention_probs, value_states)
-                hidden_states = self.reshape_batch_dim_to_heads(hidden_states)
 
+        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)
         hidden_states = self.proj_attn(hidden_states)
         return self.dropout_layer(hidden_states, deterministic=deterministic)
 
